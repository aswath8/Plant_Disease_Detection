{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit ('ericsson': conda)",
   "metadata": {
    "interpreter": {
     "hash": "13c7092849be380cec23967dfd3c0ced7623c9fe77a6057e2e8014c5b58f7891"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Searching Twitter for tweets with HashTag:  #save_my_plant\n",
      "Searching for Date since(The Tweets are required in yyyy-mm--dd)\n",
      "reply of tweet:Provide more  water.\n",
      "reply of tweet:Maybe try adding more potassium fertilizer.\n",
      "\n",
      "Tweet 1:\n",
      "Username:xuxedu\n",
      "Description:\n",
      "Location:\n",
      "Following Count:0\n",
      "Follower Count:0\n",
      "Total Tweets:6\n",
      "Retweet Count:0\n",
      "Tweet Text:My plant is affected by some disease, please provide me with some solution. #save_my_plant https://t.co/2UpAoVAcz4\n",
      "Hashtags Used:['save_my_plant']\n",
      "Tweet ID:1370295935947788295\n",
      "Replies:['Provide more  water.', 'Maybe try adding more potassium fertilizer.']\n",
      "Images:  http://pbs.twimg.com/media/EwRE4g4VoAErSPD.jpg\n",
      "Scraping has completed!\n"
     ]
    }
   ],
   "source": [
    "# Python Script to Extract tweets of a \n",
    "# particular Hashtag using Tweepy and Pandas \n",
    "\n",
    "\n",
    "# import modules \n",
    "import pandas as pd \n",
    "import tweepy \n",
    "from PIL import Image\n",
    "import urllib.request as urllib2\n",
    "\n",
    "tweet_image=None\n",
    "# function to display data of each tweet \n",
    "def printtweetdata(n, ith_tweet): \n",
    "\tglobal tweet_image\n",
    "\tprint() \n",
    "\tprint(f\"Tweet {n}:\") \n",
    "\tprint(f\"Username:{ith_tweet[0]}\") \n",
    "\tprint(f\"Description:{ith_tweet[1]}\") \n",
    "\tprint(f\"Location:{ith_tweet[2]}\") \n",
    "\tprint(f\"Following Count:{ith_tweet[3]}\") \n",
    "\tprint(f\"Follower Count:{ith_tweet[4]}\") \n",
    "\tprint(f\"Total Tweets:{ith_tweet[5]}\") \n",
    "\tprint(f\"Retweet Count:{ith_tweet[6]}\") \n",
    "\tprint(f\"Tweet Text:{ith_tweet[7]}\") \n",
    "\tprint(f\"Hashtags Used:{ith_tweet[8]}\") \n",
    "\tprint(f\"Tweet ID:{ith_tweet[9]}\") \n",
    "\tprint(f\"Replies:{ith_tweet[11]}\")\n",
    "\t\n",
    "\timages = ith_tweet[10]\n",
    "\tfor image in images:\n",
    "\t\tmedia_url=image['media_url']\n",
    "\t\tprint(\"Images: \", media_url)\n",
    "\t\ttweet_image = Image.open(urllib2.urlopen(media_url))\n",
    "\t\t#tweet_image.show()\n",
    "\n",
    "\n",
    "\n",
    "# function to perform data extraction \n",
    "def scrape(words, date_since, numtweet): \n",
    "\tglobal replies\n",
    "\t# Creating DataFrame using pandas \n",
    "\tdb = pd.DataFrame(columns=['username', 'description', 'location', 'following', \n",
    "\t\t\t\t\t\t\t'followers', 'totaltweets', 'retweetcount', 'text', 'hashtags','tweetID','images','replies',]) \n",
    "\t\n",
    "\t# We are using .Cursor() to search through twitter for the required tweets. \n",
    "\t# The number of tweets can be restricted using .items(number of tweets) \n",
    "\ttweets = tweepy.Cursor(api.search, q=words, lang=\"en\", \n",
    "\t\t\t\t\t\tsince='2021-01-01', \n",
    "\t\t\t\t\t\ttweet_mode='extended',\n",
    "\t\t\t\t\t\tinclude_entities=True).items(numtweet) \n",
    "\t\n",
    "\t# .Cursor() returns an iterable object. Each item in \n",
    "\t# the iterator has various attributes that you can access to \n",
    "\t# get information about each tweet \n",
    "\tlist_tweets = [tweet for tweet in tweets] \n",
    "\t\n",
    "\t# Counter to maintain Tweet Count \n",
    "\ti = 1\n",
    "\t\n",
    "\t# we will iterate over each tweet in the list for extracting information about each tweet \n",
    "\tfor tweet in list_tweets: \n",
    "\t\t\n",
    "\t\treply_tweets = tweepy.Cursor(api.search, q='to:{}'.format(user_name),\n",
    "                                since_id=tweet.id, tweet_mode='extended').items()\n",
    "\n",
    "\t\tusername = tweet.user.screen_name \n",
    "\t\tdescription = tweet.user.description \n",
    "\t\tlocation = tweet.user.location \n",
    "\t\tfollowing = tweet.user.friends_count \n",
    "\t\tfollowers = tweet.user.followers_count \n",
    "\t\ttotaltweets = tweet.user.statuses_count \n",
    "\t\tretweetcount = tweet.retweet_count \n",
    "\t\thashtags = tweet.entities['hashtags'] \n",
    "\t\timages=tweet.entities['media']\n",
    "\t\ttweetId= tweet.id\n",
    "\t\treplies=[]\n",
    "\t\twhile True:\n",
    "\t\t\ttry:\n",
    "\t\t\t\treply = reply_tweets.next()\n",
    "\t\t\t\tif not hasattr(reply, 'in_reply_to_status_id_str'):\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tif reply.in_reply_to_status_id == tweetId:\n",
    "\t\t\t\t\treplies.append(reply.full_text)\n",
    "\t\t\t\t\tprint(\"reply of tweet:{}\".format(reply.full_text))\n",
    "\n",
    "\t\t\texcept tweepy.RateLimitError as e:\n",
    "\t\t\t\tlogging.error(\"Twitter api rate limit reached\".format(e))\n",
    "\t\t\t\ttime.sleep(60)\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\texcept tweepy.TweepError as e:\n",
    "\t\t\t\tprint(\"Tweepy error occured:{}\".format(e))\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\texcept StopIteration:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tprint(\"Failed while fetching replies {}\".format(e))\n",
    "\t\t\t\tbreak\n",
    "\t\t\n",
    "\n",
    "\t\t# Retweets can be distinguished by a retweeted_status attribute, \n",
    "\t\t# in case it is an invalid reference, except block will be executed \n",
    "\t\ttry: \n",
    "\t\t\ttext = tweet.retweeted_status.full_text \n",
    "\t\texcept AttributeError: \n",
    "\t\t\ttext = tweet.full_text \n",
    "\t\thashtext = list() \n",
    "\t\tfor j in range(0, len(hashtags)): \n",
    "\t\t\thashtext.append(hashtags[j]['text']) \n",
    "\t\t\n",
    "\n",
    "\t\t# Here we are appending all the extracted information in the DataFrame \n",
    "\t\tith_tweet = [username, description, location, following, \n",
    "\t\t\t\t\tfollowers, totaltweets, retweetcount, text, hashtext, tweetId, images,replies] \n",
    "\t\tdb.loc[len(db)] = ith_tweet \n",
    "\t\t\n",
    "\t\t# Function call to print tweet data on screen \n",
    "\t\tprinttweetdata(i, ith_tweet) \n",
    "\n",
    "\t\ti = i+1\n",
    "\t\n",
    "\t\n",
    "\t#Retweet solution:\n",
    "\t#api.update_status('The solution to the problem is: ', tweetId)\n",
    "\n",
    "\t#filename = 'scraped_tweets.csv'\n",
    "\n",
    "\t# we will save our database as a CSV file. \n",
    "\t#db.to_csv(filename) \n",
    "\n",
    "\n",
    "\n",
    "# Enter your own credentials obtained \n",
    "# from your developer account \n",
    "consumer_key = \"runsra61MhrLsjwgOYlANBzu9\"\n",
    "consumer_secret = \"fVChsb8y4Aq9zGqgYrINmza8klJobvkWhZdkXhZxhjxGGa4OMZ\"\n",
    "access_key = \"1364862750518730752-H5jTeNtJBbVxEg9iYpbZ3Ga7gZw5Oc\"\n",
    "access_secret = \"aOQS9MUKEJ6kpLTk5Na1XSdNdcAAuq0rdalF7Hz0lIaoa\"\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret) \n",
    "auth.set_access_token(access_key, access_secret) \n",
    "api = tweepy.API(auth)\n",
    "user_name =  'xuxedu'\n",
    "\n",
    "# Enter Hashtag and initial date \n",
    "words = \"#save_my_plant\" #Must include the pound(#) symbol to explicitly serach for only hashtags.\n",
    "print(\"Searching Twitter for tweets with HashTag: \", words) \n",
    "date_since = '2021-01-01' #input() \n",
    "print(\"Searching for Date since(The Tweets are required in yyyy-mm--dd)\") \n",
    "replies=[]\n",
    "\n",
    "# number of tweets you want to extract in one run \n",
    "numtweet = 5\n",
    "scrape(words, date_since, numtweet) \n",
    "print('Scraping has completed!') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.4.0\n",
      "2.4.0\n",
      "[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Tomato_Early_blight\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow import keras\n",
    "import cv2\n",
    "from os import listdir\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation, Flatten, Dropout, Dense\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def convert_image_to_array(image_dir):\n",
    "    try:\n",
    "        image = cv2.imread(image_dir)\n",
    "        if image is not None :\n",
    "            image = cv2.resize(image, default_image_size)   \n",
    "            return img_to_array(image)\n",
    "        else :\n",
    "            return np.array([])\n",
    "    except Exception as e:\n",
    "        print(f\"Error : {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def inference(image):\n",
    "  model = keras.models.load_model('my_h5_model.h5')\n",
    "  classes=['Pepper__bell___Bacterial_spot', 'Pepper__bell___healthy',\n",
    " 'Potato___Early_blight', 'Potato___Late_blight', 'Potato___healthy',\n",
    " 'Tomato_Bacterial_spot', 'Tomato_Early_blight', 'Tomato_Late_blight',\n",
    " 'Tomato_Leaf_Mold', 'Tomato_Septoria_leaf_spot',\n",
    " 'Tomato_Spider_mites_Two_spotted_spider_mite', 'Tomato__Target_Spot',\n",
    " 'Tomato__Tomato_YellowLeaf__Curl_Virus' ,'Tomato__Tomato_mosaic_virus',\n",
    " 'Tomato_healthy']\n",
    "  img=image.reshape(1,256,256,3)\n",
    "  value=model.predict(img)\n",
    "  ind=np.argmax(value)\n",
    "  print(value)\n",
    "  print(classes[ind])\n",
    "  return classes[ind]\n",
    "  \n",
    "\n",
    "print(keras.__version__)\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "#tweet_image.show()\n",
    "open_cv_image = np.array(tweet_image) \n",
    "# Convert RGB to BGR \n",
    "open_cv_image = open_cv_image[:, :, ::-1].copy() \n",
    "disease_name=inference(open_cv_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<mysql.connector.connection.MySQLConnection object at 0x00000214EC9671D0>\n",
      "Showing all databases: \n",
      "('information_schema',)\n",
      "('sql6398512',)\n",
      "Showing all tables:\n",
      "('trustedDBTable',)\n",
      "('Tomato_Early_blight', 'Add Water')\n",
      "('Tomato_Early_blight', 'Add Potassium fertilizer')\n",
      "['Add Water', 'Add Potassium fertilizer']\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "\n",
    "#Free server registered at https://www.freesqldatabase.com/account/\n",
    "mydb = mysql.connector.connect(\n",
    "  host=\"sql6.freesqldatabase.com\",\n",
    "  user=\"sql6398512\",\n",
    "  password=\"WsJANQDqVw\",\n",
    "  database=\"sql6398512\"\n",
    ")\n",
    "\n",
    "print(mydb)\n",
    "mycursor = mydb.cursor()\n",
    "\n",
    "#mycursor.execute(\"CREATE DATABASE TrustedDB\")\n",
    "\n",
    "mycursor.execute(\"SHOW DATABASES\")\n",
    "print('Showing all databases: ')\n",
    "for x in mycursor:\n",
    "  print(x)\n",
    "\n",
    "#CREATE TABLE\n",
    "#mycursor.execute(\"CREATE TABLE trustedDBTable (disease VARCHAR(255), solutions VARCHAR(255))\")\n",
    "\n",
    "#DROP TABLE IF EXISTS\n",
    "#sql = \"DROP TABLE IF EXISTS trustedDBTable\"\n",
    "#mycursor.execute(sql)\n",
    "\n",
    "mycursor.execute(\"SHOW TABLES\")\n",
    "print(\"Showing all tables:\")\n",
    "for x in mycursor:\n",
    "  print(x)\n",
    "\n",
    "#INSERT DISEASE NAME AND SOLUTION\n",
    "sql = \"INSERT INTO trustedDBTable (disease, solutions) VALUES (%s, %s)\"\n",
    "val = [(\"Tomato_Early_blight\", \"Add Water\"),\n",
    "       (\"Tomato_Early_blight\", \"Add Potassium fertilizer\")]\n",
    "\n",
    "#mycursor.execute(sql, val)\n",
    "#mycursor.executemany(sql, val)\n",
    "#mydb.commit()\n",
    "#print(mycursor.rowcount, \"record inserted.\")\n",
    "\n",
    "#QUERY FOR SOLUTIONS\n",
    "\n",
    "sql = \"SELECT * FROM trustedDBTable WHERE disease ='\"+ disease_name +\"'\"\n",
    "mycursor.execute(sql)\n",
    "myresult = mycursor.fetchall()\n",
    "\n",
    "sql_query=[]\n",
    "for x in myresult:\n",
    "  print(x)\n",
    "  sql_query.append(x[1])\n",
    "\n",
    "print(sql_query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2 record(s) deleted\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#DELETE SOLUTIONS\n",
    "sql = \"DELETE FROM trustedDBTable WHERE disease = 'Dry Leaf' OR solutions = 'Add fertilizer'\"\n",
    "mycursor.execute(sql)\n",
    "mydb.commit()\n",
    "print(mycursor.rowcount, \"record(s) deleted\")\n"
   ]
  },
  {
   "source": [
    "#https://stackoverflow.com/questions/53503049/measure-similarity-between-two-documents-using-doc2vec\n",
    "\n",
    "#More information aboout Doc2Vec: https://radimrehurek.com/gensim/models/doc2vec.html   \n",
    "# https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html\n",
    "\n",
    "\n",
    "#Code repo on Doc2Vec https://github.com/jhlau/doc2vec, model doc2vec.bin downloaded from here.\n",
    "\n",
    "from gensim.models import doc2vec\n",
    "from scipy import spatial\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "#from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "all_stopwords_gensim = STOPWORDS\n",
    "sw_list = {\"not\"}\n",
    "selected_stopwords_gensim = STOPWORDS.difference(sw_list)\n",
    "\n",
    "\n",
    "#models=['doc2vec.bin', 'GoogleNews-vectors-negative300.bin']\n",
    "#d2v_model = doc2vec.Doc2Vec.load(models[0])\n",
    "\n",
    "\n",
    "#Model downloaded from:\n",
    "#Google news Vecor: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
    "\n",
    "from gensim import models\n",
    "import gensim.downloader as api\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "model = api.load('word2vec-google-news-300') #https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#introducing-the-word2vec-model\n",
    "print(model)  # output: /home/user/gensim-data/20-newsgroups/20-newsgroups.gz\n",
    "\n",
    "# Normalizes the vectors in the word2vec class.\n",
    "model.init_sims(replace=True)  \n",
    "\n",
    "#wv = models.KeyedVectors.load_word2vec_format(wv, binary=True)\n",
    "\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 57,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Aswath\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x0000023237553B38>\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "provide water \nmaybe try adding potassium fertilizer \nadd water\nadd potassium fertilizer\n                                        provide water   \\\nprovide water                                 0.000000   \nmaybe try adding potassium fertilizer         0.511056   \n\n                                        maybe try adding potassium fertilizer   \nprovide water                                                         0.511056  \nmaybe try adding potassium fertilizer                                 0.000000   \n\n                                        add water  add potassium fertilizer\nprovide water                            0.386386                  0.461820\nmaybe try adding potassium fertilizer    0.638554                  0.183589 \n\nBest solution:  maybe try adding potassium fertilizer \n"
     ]
    }
   ],
   "source": [
    "\n",
    "user_soln = ['pour water', 'add fertilizer', 'should give more water']\n",
    "trusted_db = ['provide adequate water','add potassium fertilizer']\n",
    "\n",
    "user_soln=replies\n",
    "trusted_db=sql_query\n",
    "\n",
    "def sanitize(text):\n",
    "    #text = remove_stopwords(text)\n",
    "    text_tokens = word_tokenize(text)\n",
    "    tokens_without_sw = [word for word in text_tokens if not word in selected_stopwords_gensim]\n",
    "    text = (\" \").join(tokens_without_sw)\n",
    "    text=text.strip()\n",
    "    text=text.lower()\n",
    "    text=text.translate(string.punctuation)\n",
    "    text=text.translate(str.maketrans('', '', string.punctuation))\n",
    "    print(text) \n",
    "    return text\n",
    "\n",
    "arr=[]\n",
    "for l in user_soln:\n",
    "    arr.append(sanitize(l))\n",
    "user_soln=arr\n",
    "\n",
    "arr=[]\n",
    "for l in trusted_db:\n",
    "    arr.append(sanitize(l))\n",
    "trusted_db=arr\n",
    "\n",
    "min_dist=999\n",
    "\n",
    "def rankit(l1,l2):\n",
    "    global recommended_solution\n",
    "    matrix=[]\n",
    "    i=0\n",
    "    for s1 in l1:\n",
    "        matrix.append([])\n",
    "        for s2 in l2:\n",
    "            #calculate distance between two sentences using WMD algorithm\n",
    "            wmdist=model.wmdistance(s1, s2)\n",
    "            matrix[i].append(wmdist)\n",
    "            if wmdist<min_dist:\n",
    "                recommended_solution=s1\n",
    "        i+=1\n",
    "\n",
    "    df2 = pd.DataFrame(np.array(matrix), columns=l2, index=l1)\n",
    "    print(df2,\"\\n\")\n",
    "\n",
    "#Auto correlation with user's solutions\n",
    "rankit(user_soln,user_soln)\n",
    "\n",
    "recommended_solution=\"\"\n",
    "\n",
    "#Cross-correlation with users and trusted solutions\n",
    "rankit(user_soln,trusted_db)\n",
    "\n",
    "\n",
    "print(\"Best solution: \",recommended_solution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pour water\n",
      "add fertilizer\n",
      "cosine distance 0.4074270725250244\n",
      "WM distance = 1.272\n",
      "ipykernel_launcher:25: DeprecationWarning: Call to deprecated `wmdistance` (Method will be removed in 4.0.0, use self.wv.wmdistance() instead).\n"
     ]
    }
   ],
   "source": [
    "first_text = 'Pour some water'\n",
    "second_tdifference'\n",
    "\n",
    "def sanitize(text):\n",
    "    #text = remove_stopwords(text)\n",
    "    text_tokens = word_tokenize(text)\n",
    "    tokens_without_sw = [word for word in text_tokens if not word in selected_stopwords_gensim]\n",
    "    text = (\" \").join(tokens_without_sw)\n",
    "    text=text.strip()\n",
    "    text=text.lower()\n",
    "    text=text.translate(string.punctuation)\n",
    "    text=text.translate(str.maketrans('', '', string.punctuation))\n",
    "    print(text)\n",
    "    return text\n",
    "\n",
    "first_text=sanitize(first_text)\n",
    "second_text=sanitize(second_text)\n",
    "\n",
    "vec1 = d2v_model.infer_vector(first_text.split())\n",
    "vec2 = d2v_model.infer_vector(second_text.split())\n",
    "\n",
    "cos_distance = spatial.distance.cosine(vec1, vec2)\n",
    "print('cosine distance',cos_distance)\n",
    "\n",
    "distance = d2v_model.wmdistance(first_text, second_text)\n",
    "\n",
    "print ('WM distance = %.3f' % distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tursted_db_solutions = []\n",
    "user_solutions = []\n",
    "for ts in tursted_db_solutions:\n",
    "    for us in user_solutions:\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "info = api.info()  # show info about available models/datasets\n",
    "model = api.load(\"glove-twitter-25\")  # download the model and return as object ready for use\n",
    "model.most_similar(\"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import text\n",
    "norm_bible=\"Hello world \"\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(norm_bible)\n",
    "\n",
    "word2id = tokenizer.word_index\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "\n",
    "vocab_size = len(word2id) + 1 \n",
    "embed_size = 100\n",
    "\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in norm_bible]\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Vocabulary Sample:', list(word2id.items())[:10])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "OSError",
     "evalue": "[WinError 126] The specified module could not be found",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-f1fb4c26b98b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mskipgrams\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# generate skip-grams\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mskip_grams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mskipgrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mwid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ericsson\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ericsson\\lib\\site-packages\\keras\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlosses_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ericsson\\lib\\site-packages\\keras\\utils\\conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ericsson\\lib\\site-packages\\keras\\backend\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mset_epsilon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfloatx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mset_floatx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcast_to_floatx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ericsson\\lib\\site-packages\\keras\\backend\\load_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tensorflow'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;31m# Try and load external backend.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ericsson\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtfdev\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ericsson\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[1;31m# pylint: disable=unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ericsson\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_column\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfeature_column_lib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfeature_column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ericsson\\lib\\site-packages\\tensorflow\\python\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtf2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ericsson\\lib\\site-packages\\tensorflow\\python\\keras\\activations.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeserialize_keras_object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ericsson\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_summary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msqueeze_or_expand_dimensions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmulti_gpu_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmulti_gpu_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnp_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnp_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ericsson\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\multi_gpu_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_export\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras_export\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ericsson\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistributed_training_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtraining_arrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtraining_distributed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtraining_eager\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ericsson\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m   \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0missparse\u001b[0m  \u001b[1;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m   \u001b[0missparse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ericsson\\lib\\site-packages\\scipy\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[1;31m# Allow distributors to run custom init code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[1;31m# Filter out Cython harmless warnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ericsson\\lib\\site-packages\\scipy\\_distributor_init.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     61\u001b[0m                 \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlibs_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlibs_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'*dll'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                     \u001b[0mWinDLL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m                 \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mowd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ericsson\\lib\\ctypes\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 348\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    349\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 126] The specified module could not be found"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "\n",
    "# generate skip-grams\n",
    "skip_grams = [skipgrams(wid, vocabulary_size=vocab_size, window_size=10) for wid in wids]\n",
    "\n",
    "# view sample skip-grams\n",
    "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
    "for i in range(10):\n",
    "    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
    "          id2word[pairs[i][0]], pairs[i][0], \n",
    "          id2word[pairs[i][1]], pairs[i][1], \n",
    "          labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: https://towardsdatascience.com/detecting-document-similarity-with-doc2vec-f8289a9a7db7\n"
   ]
  }
 ]
}